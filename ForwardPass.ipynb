{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73a962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, tqdm, json\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from loss import YOLOLoss\n",
    "from models import ResNet\n",
    "from data import YOLODataset\n",
    "from utils import parse, get_metadata, intersection_over_union\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5af1b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('red_examples.json', 'r') as f:\n",
    "    red_examples = json.load(f)\n",
    "    \n",
    "train_red_book = red_examples['train_red_book']\n",
    "val_red_book = red_examples['val_red_book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d293afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [x for x in os.listdir(\"dataset/valid\") if x[-3:] == \"jpg\"]\n",
    "annotation_names = [x for x in os.listdir(\"dataset/valid\") if x[-3:] == \"xml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2946e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 458/458 [00:05<00:00, 85.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2634/2634 [00:26<00:00, 99.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 966/966 [00:10<00:00, 95.50it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for name in ['test', 'train', 'valid']:\n",
    "    file = []\n",
    "    image_names = [x for x in os.listdir(\"dataset/\" + name) if x[-3:] == \"jpg\"]\n",
    "    annotation_names = [x for x in os.listdir(\"dataset/\" + name) if x[-3:] == \"xml\"]\n",
    "    metadata = get_metadata(name, annotation_names)\n",
    "    \n",
    "    for i in range(len(metadata)):\n",
    "        row = []\n",
    "        \n",
    "        row.append(f\"dataset/{name}/\" + image_names[i])\n",
    "        row.append(metadata[i])\n",
    "        file.append(row)\n",
    "    data.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26afefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_indx = 0\n",
    "for red_idx in train_red_book:\n",
    "    data[1].pop(red_idx - d_indx)\n",
    "    d_indx += 1 \n",
    "\n",
    "\n",
    "d_indx = 0\n",
    "for red_idx in val_red_book:\n",
    "    del data[2][red_idx - d_indx]\n",
    "    d_indx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d9e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {\n",
    "    \"truck\": 1,\n",
    "    \"car\": 2,\n",
    "    \"bus\": 3\n",
    "}\n",
    "\n",
    "encoder = {\n",
    "    \"truck\": torch.tensor([1, 0, 0]),\n",
    "    \"car\": torch.tensor([0, 1, 0]),\n",
    "    \"bus\": torch.tensor([0, 0, 1]),\n",
    "}\n",
    "\n",
    "ANCHORS = torch.tensor([\n",
    "    [104, 104, 120, 127], \n",
    "    [ 28, 109,  53, 144],\n",
    "    # [100, 104, 125, 127]\n",
    "]) / 320\n",
    "\n",
    "anchors = ANCHORS[:, 2:].reshape(1, 2, 1, 1, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e6b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YOLODataset(data[1], encoder, ANCHORS, train_transforms)\n",
    "val_dataset = YOLODataset(data[2], encoder, ANCHORS, transform)\n",
    "\n",
    "trainLoader = DataLoader(train_dataset, batch_size=64)\n",
    "valLoader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ef29b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(in_channels=3, num_classes=3, layers=[2, 2, 2]).to(device)\n",
    "model.load_state_dict(torch.load(\"models/best_ModelTest.pt\")['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09e50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YOLOLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer.load_state_dict(torch.load(\"models/best_ModelTest.pt\")['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f729679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, epoch, prev_loss, current_loss, model, optimizer):\n",
    "    if prev_loss > current_loss:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": current_loss,\n",
    "        }, f\"models/best_{model_name}.pt\") # create models folder before! \n",
    "        print(\"The best model was saved!\")\n",
    "        prev_loss = current_loss\n",
    "    \n",
    "    torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": current_loss,\n",
    "        }, f\"models/last_{model_name}.pt\")\n",
    "    return prev_loss\n",
    "\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "num_epochs, prev_loss = 100, -torch.inf\n",
    "train_loss, val_loss = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac79c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                              | 1/100 [02:09<3:34:11, 129.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss 12.385187058221726 | Validation loss 11.739208936691284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                             | 2/100 [03:58<3:11:35, 117.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training loss 12.355651128859748 | Validation loss 11.026854038238525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|██▎                                                                            | 3/100 [05:49<3:05:05, 114.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training loss 12.504978043692452 | Validation loss 12.357191622257233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▏                                                                           | 4/100 [07:42<3:01:57, 113.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training loss 12.764366831098284 | Validation loss 12.171160519123077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███▉                                                                           | 5/100 [09:28<2:56:00, 111.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Training loss 12.695029485793341 | Validation loss 12.584539532661438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▋                                                                          | 6/100 [11:29<2:59:33, 114.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Training loss 13.338673023950486 | Validation loss 12.966343879699707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                         | 7/100 [13:24<2:57:50, 114.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Training loss 12.838110560462589 | Validation loss 12.140791594982147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▎                                                                        | 8/100 [15:20<2:56:15, 114.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Training loss 12.424103328159877 | Validation loss 11.521124303340912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████                                                                        | 9/100 [17:14<2:53:53, 114.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Training loss 12.619962873912993 | Validation loss 11.60217970609665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████▊                                                                      | 10/100 [19:02<2:48:53, 112.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Training loss 12.64015143258231 | Validation loss 11.939994513988495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████████▌                                                                     | 11/100 [20:50<2:44:54, 111.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Training loss 12.390050751822335 | Validation loss 11.460948288440704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▎                                                                    | 12/100 [22:38<2:41:51, 110.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Training loss 13.400397459665934 | Validation loss 11.827004134654999\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(num_epochs):\n",
    "    model.train()\n",
    "    loss_list, count = 0, 0\n",
    "    for images, targets in trainLoader:\n",
    "        images = images.to(device, dtype=torch.float) # .permute(0, 3, 1, 2)\n",
    "        # targets = targets\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out1, out2 = model(images)\n",
    "        \n",
    "        # loss = criterion(features, output, targets)\n",
    "        loss = criterion(out1.reshape(-1, 2, 40, 40, 8), targets[0].to(device), anchors) + criterion(out2.reshape(-1, 2, 20, 20, 8), targets[1].to(device), anchors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list += loss.item()\n",
    "        count += 1\n",
    "    loss = loss_list/count\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "    model.eval()\n",
    "    @torch.no_grad()\n",
    "    def validation():\n",
    "        val_loss, val_counter = 0, 0\n",
    "        for images, targets in valLoader:\n",
    "            images = images.permute(0, 3, 1, 2).to(device, dtype=torch.float) # .permute(0, 3, 1, 2)\n",
    "            # targets = targets.to(device)\n",
    "            \n",
    "            out1, out2 = model(images)\n",
    "            \n",
    "            loss = criterion(out1.reshape(-1, 2, 40, 40, 8), targets[0].to(device), anchors) + criterion(out2.reshape(-1, 2, 20, 20, 8), targets[1].to(device), anchors)\n",
    "            val_loss += loss.item()\n",
    "            val_counter += 1\n",
    "        return val_loss / val_counter\n",
    "    v_loss = validation()\n",
    "    val_loss.append(v_loss)\n",
    "    prev_loss = save_model(\"ModelTest\", epoch + 100, prev_loss, v_loss, model, optimizer)\n",
    "    \n",
    "    print(f\"Epoch: {epoch} | Training loss {loss} | Validation loss {v_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eaa4d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 40, 40])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f539000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 40, 40, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842fe4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7512b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f10809f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 320, 320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478bf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f5fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d709f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yerda\\anaconda3\\lib\\site-packages\\albumentations\\imgaug\\transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
      "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "IMAGE_SIZE = 320\n",
    "scale = 1.1\n",
    "\n",
    "transform = A.Compose([\n",
    "                A.Blur(blur_limit=50, p=0.1),\n",
    "                A.MedianBlur(blur_limit=51, p=0.1),\n",
    "                A.ToGray(p=0.3)],\n",
    "                bbox_params=A.BboxParams(format='yolo', label_fields=[]))\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE * scale),\n",
    "            min_width=int(IMAGE_SIZE * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.IAAAffine(shear=15, p=0.5, mode=\"constant\"),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        # A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299c9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96893b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a2750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fff5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71629f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = [(0.555230164527893, 0.378763222694397, 0.035061979293823264, 0.034236288070678744), (0.7444658160209656, 0.3889347195625305, 0.0419869661331177, 0.03695943355560305), (0.4715870380401611, 0.4050339698791504, 0.045640659332275346, 0.047342491149902355), (0.6916573882102967, 0.410683798789978, 0.04542219638824463, 0.03703227043151858), (0.7938147485256195, 0.4471988916397095, 0.05605548620223999, 0.05271601676940918), (0.2944324016571045, 0.4784142255783081, 0.06723499298095703, 0.0941751956939697), (0.36526331901550296, 0.49625308513641353, 0.10595073699951174, 0.138794469833374), (0.4546086072921753, 0.5768776655197143, 0.0877914905548095, 0.09203457832336426), (0.41252171993255615, 0.780198049545288, 0.16893115043640128, 0.19423360824584968), (0.8886498898267746, 0.7868667840957642, 0.17154697775840755, 0.15564322471618652)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "578fd44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5552, 0.3788, 0.0351, 0.0342],\n",
       "         [0.7445, 0.3889, 0.0420, 0.0370],\n",
       "         [0.4716, 0.4050, 0.0456, 0.0473],\n",
       "         [0.6917, 0.4107, 0.0454, 0.0370],\n",
       "         [0.7938, 0.4472, 0.0561, 0.0527],\n",
       "         [0.2944, 0.4784, 0.0672, 0.0942],\n",
       "         [0.3653, 0.4963, 0.1060, 0.1388],\n",
       "         [0.4546, 0.5769, 0.0878, 0.0920],\n",
       "         [0.4125, 0.7802, 0.1689, 0.1942],\n",
       "         [0.8886, 0.7869, 0.1715, 0.1556]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(q)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Добрый вечер!\n",
    "Почему albumentations ошибку показывает хотя bboxes в формате yolo and normalized? \n",
    "Кто-нибудь знает как решить эту проблему? \n",
    "\n",
    "\n",
    "аугментировать получается до определенного прямоугольника"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
